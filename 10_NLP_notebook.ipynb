{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XuanJanice/Calculator-Program/blob/main/10_NLP_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "984fab4a-284d-45ad-b1e6-161107350b89",
      "metadata": {
        "id": "984fab4a-284d-45ad-b1e6-161107350b89"
      },
      "source": [
        "## Natural Language Processing\n",
        "\n",
        "* NLP is performed on **text collections** (**corpora**, plural of **corpus**):\n",
        "    * Tweets\n",
        "    * Facebook posts\n",
        "    * Conversations\n",
        "    * Movie reviews\n",
        "    * Documents\n",
        "    * Books\n",
        "    * News\n",
        "    * And more\n",
        "* **Nuances of meaning** make natural language understanding difficult \n",
        "    * Text’s meaning can be influenced by **context** and **reader’s “world view”** \n",
        "\n",
        "\n",
        "natural language toolkit (nltk)\n",
        "\n",
        "Python nltk package has modules for working with human language data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Extracting text from Web - Web Scraping"
      ],
      "metadata": {
        "id": "HVGLC-k8XAxR"
      },
      "id": "HVGLC-k8XAxR"
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "url = \"https://www.techtarget.com/searchsap/news/252515342/Unilever-pursues-supply-chain-sustainability-with-blockchain\"\n",
        "#url = \"https://edition.cnn.com/2020/09/13/tech/microsoft-tiktok-bytedance/index.html\"\n",
        "\n",
        "response = requests.get(url)\n",
        "#print(response.url, \"\\n\")\n",
        "\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# remove unnecessory elements of HTML tree\n",
        "for s in soup(['style', 'script', '[document]', 'head', 'title']):\n",
        "  s.extract()\n",
        "\n",
        "#now get the text\n",
        "text = soup.get_text()\n",
        "text"
      ],
      "metadata": {
        "id": "H0yN6g3SIHkN"
      },
      "id": "H0yN6g3SIHkN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2e302525-6db1-4a07-8f82-9660d9bca029",
      "metadata": {
        "id": "2e302525-6db1-4a07-8f82-9660d9bca029"
      },
      "source": [
        "## 2 - Tokenizing text \n",
        "\n",
        "### TextBlob\n",
        "\n",
        "* https://textblob.readthedocs.io/en/latest/\n",
        "* Object-oriented NLP text-processing library that is built on the NLTK and pattern NLP libraries\n",
        "* TextBlob is a Python library for processing textual data.\n",
        "* One of the tasks **TextBlob** can perform is:\n",
        "    * Tokenization—splitting text into pieces called tokens, which are meaningful units, such as words and numbers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "132c926e-1616-4be8-9e7c-4ac443751764",
      "metadata": {
        "id": "132c926e-1616-4be8-9e7c-4ac443751764"
      },
      "outputs": [],
      "source": [
        "!pip install -U textblob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8e88608-3dbc-42cf-85fc-dea7155ad600",
      "metadata": {
        "id": "a8e88608-3dbc-42cf-85fc-dea7155ad600"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "import nltk #NLTK, or Natural Language Toolkit, is a Python package that used for NLP\n",
        "nltk.download('punkt') #Punkt Sentence Tokenizer - This tokenizer divides a text into a list of sentences\n",
        "  \n",
        "blob = TextBlob(text)\n",
        "\n",
        "tokens = blob.words\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd605060-8212-4f54-87b2-81b9c3aff694",
      "metadata": {
        "id": "fd605060-8212-4f54-87b2-81b9c3aff694"
      },
      "source": [
        "### Another approach to tokenize text - Tokenize using NLTK Regular-Expression Tokenizers\n",
        "\n",
        "https://www.nltk.org/api/nltk.tokenize.regexp.html\n",
        "\n",
        "https://www.nltk.org/_modules/nltk/tokenize/regexp.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bbc499c-1817-4fce-94b3-caf75816a7f8",
      "metadata": {
        "id": "5bbc499c-1817-4fce-94b3-caf75816a7f8"
      },
      "outputs": [],
      "source": [
        "#Alternative\n",
        "\n",
        "# Creating a tokenizer\n",
        "tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n",
        "\n",
        "# Tokenizing the text\n",
        "tokensReg = tokenizer.tokenize(text)\n",
        "tokensReg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "894aac41-4a8f-495d-ba6b-9229635515a3",
      "metadata": {
        "id": "894aac41-4a8f-495d-ba6b-9229635515a3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 - Remove common words (`stopwords`) from text"
      ],
      "metadata": {
        "id": "sasycccbORRs"
      },
      "id": "sasycccbORRs"
    },
    {
      "cell_type": "markdown",
      "id": "5debf7d8-670a-41ee-bf17-706f2903c02b",
      "metadata": {
        "id": "5debf7d8-670a-41ee-bf17-706f2903c02b"
      },
      "source": [
        "### Getting the English stop words from nltk\n",
        "\n",
        " `Stopwords` are common words which provide little to no value to the meaning of the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3684e85-cb6a-419a-8a25-2a8ab07e2c50",
      "metadata": {
        "id": "e3684e85-cb6a-419a-8a25-2a8ab07e2c50"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aad65a68-706d-4987-a995-e29111c52eb8",
      "metadata": {
        "id": "aad65a68-706d-4987-a995-e29111c52eb8"
      },
      "outputs": [],
      "source": [
        "## Remove stop words in the text ##\n",
        "filtered_text = []\n",
        "\n",
        "for w in tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_text.append(w)\n",
        "        \n",
        "filtered_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "053479c9-c421-43f4-ad15-dc038f902404",
      "metadata": {
        "id": "053479c9-c421-43f4-ad15-dc038f902404"
      },
      "source": [
        "## 4 - **Counter** to count objects \n",
        "In collections, you’ll find a class specially designed to count several different objects in one go. This class is conveniently called **Counter**.\n",
        "\n",
        "https://docs.python.org/3/library/collections.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b183429-3402-41db-af4d-ca80b47244c5",
      "metadata": {
        "id": "4b183429-3402-41db-af4d-ca80b47244c5"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# generates a collection where elements are stored as dictionary keys and their counts are stored as dictionary values - key:value pairs.\n",
        "item_count = Counter(filtered_text)\n",
        "print(item_count, \"\\n\")\n",
        "\n",
        "# Converts the list to a list of (elements, counter) pairs\n",
        "text_items = item_count.items() \n",
        "\n",
        "# Return a list of the n most common elements and their counts from the most common to the least.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 - Storing text items in a pandas Dataframe & Plot graph"
      ],
      "metadata": {
        "id": "6LnIvtcURtAL"
      },
      "id": "6LnIvtcURtAL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55062ac5-7e83-4299-a37e-0b46a7b191e9",
      "metadata": {
        "id": "55062ac5-7e83-4299-a37e-0b46a7b191e9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Create DataFrame and Storing it in variable df\n",
        "\n",
        "\n",
        "## Visualise the Dataframe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 - Generate a simple WordCloud\n",
        "* https://towardsdatascience.com/simple-wordcloud-in-python-2ae54a9f58e5/\n",
        "* https://www.geeksforgeeks.org/generating-word-cloud-python/\n"
      ],
      "metadata": {
        "id": "Kvh03g1bNOlP"
      },
      "id": "Kvh03g1bNOlP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9e9c6c1-46d2-47c6-868b-719310baf3e1",
      "metadata": {
        "id": "c9e9c6c1-46d2-47c6-868b-719310baf3e1"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Create and generate a word cloud image\n",
        "\n",
        "\n",
        "#set figure size\n",
        "plt.figure(figsize=(10, 20))\n",
        "#show image\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "984fab4a-284d-45ad-b1e6-161107350b89"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}